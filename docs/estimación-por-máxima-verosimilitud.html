<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 4 Estimación por máxima verosimilitud | Notas Curso de Estadística (Parte I)</title>
  <meta name="description" content="Capítulo 4 Estimación por máxima verosimilitud | Notas Curso de Estadística (Parte I)" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 4 Estimación por máxima verosimilitud | Notas Curso de Estadística (Parte I)" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 4 Estimación por máxima verosimilitud | Notas Curso de Estadística (Parte I)" />
  
  
  

<meta name="author" content="Maikol Solís" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="densidades-previas-conjugadas-y-estimadores-de-bayes.html"/>
<link rel="next" href="propiedades-del-mle.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Curso de Estadística</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introducción</a></li>
<li class="chapter" data-level="2" data-path="inferencia-estadística.html"><a href="inferencia-estadística.html"><i class="fa fa-check"></i><b>2</b> Inferencia estadística</a>
<ul>
<li class="chapter" data-level="2.1" data-path="inferencia-estadística.html"><a href="inferencia-estadística.html#ejemplo"><i class="fa fa-check"></i><b>2.1</b> Ejemplo</a></li>
<li class="chapter" data-level="2.2" data-path="inferencia-estadística.html"><a href="inferencia-estadística.html#modelo-estadístico"><i class="fa fa-check"></i><b>2.2</b> Modelo estadístico</a></li>
<li class="chapter" data-level="2.3" data-path="inferencia-estadística.html"><a href="inferencia-estadística.html#estadístico"><i class="fa fa-check"></i><b>2.3</b> Estadístico</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><i class="fa fa-check"></i><b>3</b> Densidades previas conjugadas y estimadores de Bayes</a>
<ul>
<li class="chapter" data-level="3.1" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#distribución-previa-distribución-a-priori"><i class="fa fa-check"></i><b>3.1</b> Distribución previa (distribución a priori)</a></li>
<li class="chapter" data-level="3.2" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#densidad-posterior"><i class="fa fa-check"></i><b>3.2</b> Densidad posterior</a></li>
<li class="chapter" data-level="3.3" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#proceso-de-modelación-de-parámetros."><i class="fa fa-check"></i><b>3.3</b> Proceso de modelación de parámetros.</a></li>
<li class="chapter" data-level="3.4" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#función-de-verosimilitud"><i class="fa fa-check"></i><b>3.4</b> Función de verosimilitud</a></li>
<li class="chapter" data-level="3.5" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#familias-conjugadas"><i class="fa fa-check"></i><b>3.5</b> Familias conjugadas</a></li>
<li class="chapter" data-level="3.6" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#densidades-previas-impropias"><i class="fa fa-check"></i><b>3.6</b> Densidades previas impropias</a></li>
<li class="chapter" data-level="3.7" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#funciones-de-pérdida"><i class="fa fa-check"></i><b>3.7</b> Funciones de pérdida</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#función-de-pérdida-cuadrática"><i class="fa fa-check"></i><b>3.7.1</b> Función de pérdida cuadrática</a></li>
<li class="chapter" data-level="3.7.2" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#función-de-pérdida-absoluta"><i class="fa fa-check"></i><b>3.7.2</b> Función de pérdida absoluta</a></li>
<li class="chapter" data-level="3.7.3" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#otras-funciones-de-pérdida"><i class="fa fa-check"></i><b>3.7.3</b> Otras funciones de pérdida</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#efecto-de-muestras-grandes"><i class="fa fa-check"></i><b>3.8</b> Efecto de muestras grandes</a></li>
<li class="chapter" data-level="3.9" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#consistencia"><i class="fa fa-check"></i><b>3.9</b> Consistencia</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="estimación-por-máxima-verosimilitud.html"><a href="estimación-por-máxima-verosimilitud.html"><i class="fa fa-check"></i><b>4</b> Estimación por máxima verosimilitud</a></li>
<li class="chapter" data-level="5" data-path="propiedades-del-mle.html"><a href="propiedades-del-mle.html"><i class="fa fa-check"></i><b>5</b> Propiedades del MLE</a>
<ul>
<li class="chapter" data-level="5.1" data-path="propiedades-del-mle.html"><a href="propiedades-del-mle.html#propiedad-de-invarianza"><i class="fa fa-check"></i><b>5.1</b> Propiedad de invarianza</a></li>
<li class="chapter" data-level="5.2" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#consistencia"><i class="fa fa-check"></i><b>5.2</b> Consistencia</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="cálculo-numérico.html"><a href="cálculo-numérico.html"><i class="fa fa-check"></i><b>6</b> Cálculo numérico</a>
<ul>
<li class="chapter" data-level="6.1" data-path="cálculo-numérico.html"><a href="cálculo-numérico.html#método-de-los-momentos"><i class="fa fa-check"></i><b>6.1</b> Método de los momentos</a></li>
<li class="chapter" data-level="6.2" data-path="cálculo-numérico.html"><a href="cálculo-numérico.html#método-delta"><i class="fa fa-check"></i><b>6.2</b> Método Delta</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="estadísticos-suficientes-y-criterio-de-factorización.html"><a href="estadísticos-suficientes-y-criterio-de-factorización.html"><i class="fa fa-check"></i><b>7</b> Estadísticos Suficientes y Criterio de Factorización</a></li>
<li class="chapter" data-level="8" data-path="estadísticos-suficientes.html"><a href="estadísticos-suficientes.html"><i class="fa fa-check"></i><b>8</b> Estadísticos suficientes</a>
<ul>
<li class="chapter" data-level="8.1" data-path="estadísticos-suficientes.html"><a href="estadísticos-suficientes.html#teorema-de-factorización-de-fisher"><i class="fa fa-check"></i><b>8.1</b> Teorema de Factorización de Fisher</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="estadístico-suficiente-multivariado-.html"><a href="estadístico-suficiente-multivariado-.html"><i class="fa fa-check"></i><b>9</b> Estadístico suficiente multivariado.</a></li>
<li class="chapter" data-level="10" data-path="estadísticos-minimales.html"><a href="estadísticos-minimales.html"><i class="fa fa-check"></i><b>10</b> Estadísticos minimales</a></li>
<li class="chapter" data-level="11" data-path="mejorando-estimadores.html"><a href="mejorando-estimadores.html"><i class="fa fa-check"></i><b>11</b> Mejorando estimadores</a></li>
<li class="chapter" data-level="12" data-path="distribución-muestral-de-un-estadístico.html"><a href="distribución-muestral-de-un-estadístico.html"><i class="fa fa-check"></i><b>12</b> Distribución muestral de un estadístico</a></li>
<li class="chapter" data-level="13" data-path="distribución-muestral.html"><a href="distribución-muestral.html"><i class="fa fa-check"></i><b>13</b> Distribución muestral</a></li>
<li class="chapter" data-level="14" data-path="distribución-chi2.html"><a href="distribución-chi2.html"><i class="fa fa-check"></i><b>14</b> Distribución <span class="math inline">\(\chi^2\)</span></a>
<ul>
<li class="chapter" data-level="14.1" data-path="distribución-chi2.html"><a href="distribución-chi2.html#distribución-t"><i class="fa fa-check"></i><b>14.1</b> Distribución <span class="math inline">\(t\)</span></a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notas Curso de Estadística (Parte I)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="estimación-por-máxima-verosimilitud" class="section level1" number="4">
<h1><span class="header-section-number">Capítulo 4</span> Estimación por máxima verosimilitud</h1>
<p>¿Será posible estimar sin una densidad previa? Se debería ajustar la noción de muestra a independencia dado el valor de un parámetro.</p>
<p>Recuerde que, para <span class="math inline">\(X_1,\dots, X_n \stackrel{i.i.d}{\sim} f(X|\theta)\)</span> con <span class="math inline">\(\theta\)</span> fijo, la <strong>función de verosimilitud</strong> se define como
<span class="math display">\[ f_n(X|\theta) = \pi(X_i|\theta) = G(\theta|X).\]</span></p>
<p>Si <span class="math inline">\(\theta_1,\theta_2\in \Omega\)</span>, <span class="math inline">\(\theta\)</span> es el valor real del parámetro. Si la muestra es fija, evaluamos, para <span class="math inline">\(\theta_1\)</span>, <span class="math inline">\(f_n(X|\theta_1) = G(\theta_1|X)\)</span> y, de igual forma para <span class="math inline">\(\theta_2\)</span>, <span class="math inline">\(f_n(X|\theta_2) = G(\theta_2|X)\)</span>. Supongamos que
<span class="math display">\[f_n(X|\theta_1) &gt;f_n(X|\theta_2) \implies G(\theta_1|X)&gt;G(\theta_2|X) \text{ (principio de verosimilitud)}\]</span></p>
<p><strong>Interpretación</strong>. Es más verosímil (realista) que el verdadero parámetro sea <span class="math inline">\(\theta_1\)</span> que <span class="math inline">\(\theta_2\)</span> dada la muestra.</p>
<p><strong>Definición</strong>. Para cada <span class="math inline">\(x\in \mathcal{X}\)</span> (espacio muestral), sea <span class="math inline">\(\delta(x) \in \delta\)</span> estimador de <span class="math inline">\(\theta\)</span> tal que <span class="math inline">\(f_n(x|\theta)\)</span> es máximo. A <span class="math inline">\(\delta(x)\)</span> se le llama <strong>MLE (estimador de máxima verosimilitud)</strong>.</p>
<p><strong>Ejemplo</strong>. Si <span class="math inline">\(X_1,\dots, X_n \sim \text{Exp}(\theta)\)</span>, estime <span class="math inline">\(\theta\)</span>.</p>
<p>Determinamos la función de verosimilitud,
<span class="math display">\[f_n(X|\theta) = \prod_{i=1}^n \dfrac{1}\theta e^{-X_i/\theta} = \dfrac1{\theta^n} \exp\left(\dfrac{1}\theta \sum_{i=1}^nX_i\right) = \theta^{-n}e^{-y/\theta}.\]</span></p>
<p>Considere la <strong>log-verosimilitud</strong></p>
<p><span class="math display">\[L(\theta|X) = \ln f_n(X|\theta) = -n\ln \theta - \dfrac{y}{\theta}\]</span></p>
<p>Como es una transformación monótona creciente, la función de verosimilitud se maximiza si la log-verosimilitud es máxima. Entonces,</p>
<p><span class="math display">\[\dfrac{\partial}{\partial\theta} L(\theta|X) = \dfrac{-n}{\theta}+\dfrac{y}{\theta^2} = 0\implies \dfrac{1}{\theta}\left(-n+\dfrac{y}\theta\right)=0 \implies \hat\theta = \dfrac{y}{n} = \bar X_n.\]</span>
Para verificar que es un máximo:</p>
<p><span class="math display">\[\dfrac{\partial^2 L}{\partial\theta^2} = \left. \dfrac{n}{\theta^2} -\dfrac{2y}{\theta^3}\right\vert_{\theta = \frac{y}{n}} = \dfrac{1}{\hat\theta^2} \bigg[n-\dfrac{2y}{\frac yn}\bigg] = \dfrac{-n}{\hat\theta^2} &lt; 0.\]</span></p>
<p>Entonces <span class="math inline">\(\hat\theta = \bar X_n\)</span> es el MLE de <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Ejemplo</strong>. En una prueba sobre alguna enfermedad, en un <span class="math inline">\(90\%\)</span> da la verdadera condición (enfermo) y en un <span class="math inline">\(10\%\)</span> la prueba se equivoca (que diga que la persona esté enferma cuando está sana). Considere una variable aleatoria <span class="math inline">\(\text{Bernoulli}(\theta)\)</span>,<span class="math inline">\(\theta \in \{0.9,0.1\}\)</span>
Una muestra sería
<span class="math display">\[x = \begin{cases}1 &amp; \text{si la prueba es positiva}\\0&amp; \text{si no}\end{cases}\]</span>
Si <span class="math inline">\(x=0\)</span>, entonces <span class="math inline">\(f(0|\theta) = \begin{cases}0.9 &amp; \text{si }\theta = 0.1\\0.1&amp; \text{si }\theta = 0.9\end{cases}\)</span>.</p>
<p>Si <span class="math inline">\(x=1\)</span>, entonces <span class="math inline">\(f(1|\theta) = \begin{cases}0.1 &amp; \text{si }\theta = 0.1\\0.9&amp; \text{si }\theta = 0.9\end{cases}\)</span>.</p>
<p>El MLE corresponde a
<span class="math display">\[\hat\theta = \begin{cases}0.1 &amp; \text{si }x= 0\\0.9&amp; \text{si }x= 1\end{cases}\]</span>
<strong>Ejemplo</strong>. Para el caso normal, <span class="math inline">\(X_1,\dots, X_n \sim N(\mu,\sigma^2)\)</span>, <span class="math inline">\(\sigma^2\)</span> conocida, estime <span class="math inline">\(\mu\)</span>.</p>
<p><span class="math display">\[f_n(x|\mu) = \prod_{i=1}^n \dfrac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\dfrac{(x_i-\mu)^2}{2\sigma^2}\right) = (2\pi\sigma^2)^{-n/2}\exp\left(-\dfrac1{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\right).\]</span></p>
<p>La log-verosimilitud es de la forma
<span class="math display">\[ L(\mu|x) = \dfrac{-n}{2}\ln(2\pi\sigma^2)-\dfrac1{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2.\]</span></p>
<p>Basta con minimizar <span class="math inline">\(Q(\mu) = \sum_{i=1}^n(x_i-\mu)^2\)</span>.</p>
<p><span class="math display">\[ \dfrac{\partial Q}{\partial\mu} = -2\sum_{i=1}^n(x_i-\mu) \implies n\mu = \sum_{i=1}^nx_i \implies \hat\mu = \bar x_n.\]</span></p>
<p>No hace falta verificar la condición de segundo orden, pues <span class="math inline">\(Q\)</span> es una función cuadrática de <span class="math inline">\(\mu\)</span> y tiene un único máximo.</p>
<p><span class="math display">\[ \hat\mu_{MLE} = \bar x_n \quad (*)\]</span></p>
<p>Ahora, si <span class="math inline">\(X_1,\dots, X_n \sim N(\mu,\sigma^2)\)</span>, <span class="math inline">\(\theta = (\mu,\sigma^2)\)</span> desconocido, por <span class="math inline">\((*)\)</span>,
<span class="math display">\[ L(\sigma^2|X_1,\dots, X_n) = \dfrac n2 \ln(2\pi\sigma^2)--\dfrac1{2\sigma^2}\sum_{i=1}^n(x_i-\bar x_n)^2\]</span></p>
<p><span class="math display">\[ \dfrac{\partial L}{\partial\sigma^2} = -\dfrac n2 \dfrac1{2\pi\sigma^2} + \dfrac1{2(\sigma^2)^2} \sum_{i=1}^n(x_i-\bar x_n)^2= 0 \]</span>
Entonces
<span class="math display">\[ \sigma^2 = \dfrac 1n \sum_{i=1}^n(x_i-\mu)^2 \text{ (varianza muestral)}\]</span></p>
<p>Las condiciones de segundo orden quedan como ejercicio.</p>
<p><strong>Nota</strong>. Si <span class="math inline">\(\theta_{MLE}\)</span> de <span class="math inline">\(\theta\)</span>, entonces <span class="math inline">\(h(\theta_{MLE})\)</span> es el MLE de <span class="math inline">\(h(\theta)\)</span>.</p>
<p>Sea <span class="math inline">\(h(x,y) = \sqrt{y}\)</span> (es inyectiva). <span class="math inline">\(h(\bar x_n, \hat\sigma^2) = \sqrt{\hat\sigma^2} = \hat\sigma\)</span>.</p>
<p>El MLE de <span class="math inline">\(\dfrac{\sigma}{\mu} = \dfrac{\hat \sigma}{\bar x_n}\)</span>.</p>
<p><strong>Ejemplo</strong>. <span class="math inline">\(X_1,\dots, X_n \stackrel{i.i.d}{\sim} \text{Unif}(0)\)</span>. Estime <span class="math inline">\(\theta\)</span> <span class="math inline">\((\theta &gt; 0)\)</span>. Suponga que <span class="math inline">\(x_i&gt;0 \forall i\)</span>.</p>
<p><span class="math display">\[f(X|\theta) = \dfrac 1\theta \cdot 1_{[0,\theta]}(x)\]</span></p>
<p>La verosimilitud es
<span class="math display">\[f_n(x|\theta) = \prod_{i=1}^{n} f(x_i|\theta) = \dfrac 1{\theta^n} \prod_{i=1}^n 1_{\{0\leq x_i\leq \theta\}} \quad 0\leq x_i \leq \theta \;\forall i\]</span>
Vea que <span class="math inline">\(f_n(x|\theta)\)</span> es positivo si y solo si <span class="math inline">\(O\leq X_{(n)}\leq \theta\)</span>.</p>
<p>El valor de la muestra <span class="math inline">\(\{X_1,\dots, X_n\}\)</span> en la <span class="math inline">\(i\)</span>-ésima posición cuando los datos se ordenan de menor a mayor se denota <span class="math inline">\(X_{(i)}\)</span> (estadístico de orden). En este caso, <span class="math inline">\(X_{(n)} = \max\{X_1,\dots, X_n\}\)</span>.
Entonces <span class="math inline">\(\hat\theta_{MLE} = x_{(n)}\)</span>.</p>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="propiedades-del-mle.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/maikol-solis/notas-curso-estadistica-parte-1/edit/master/03-maxima-verosimilitud.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/maikol-solis/notas-curso-estadistica-parte-1/blob/master/03-maxima-verosimilitud.Rmd",
"text": null
},
"download": ["Notas-Curso-Estadistica.pdf"],
"toc": {
"collapse": "subsection"
},
"toc_depth": 5
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
